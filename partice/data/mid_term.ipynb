{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RSoJ_fICVb4"
   },
   "source": [
    "1. 데이터 불러오기/정보/결측치 확인\n",
    "2. 데이터 전처리\n",
    "3. 훈련/테스트 세트로 데이터 분리\n",
    "4. 모델 컴파일 및 학습\n",
    "5. 손실 및 정확도,예측 등 결과 확인 및 분석\n",
    "6. 성능 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51zuYOsfPQ96"
   },
   "source": [
    "1-1. 기본적인 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XJ3fpWQZCLBo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlJ8rBAqPXo_"
   },
   "source": [
    "1-2. CSV 파일 불러오고, 파일 정보 및 이상치 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "aa-ALuU7Pgzg",
    "outputId": "45d14236-97c1-42e8-8b3a-8607ef2be122"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>4172</td>\n",
       "      <td>F</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>4173</td>\n",
       "      <td>M</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>4174</td>\n",
       "      <td>M</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.3080</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>4175</td>\n",
       "      <td>F</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>4176</td>\n",
       "      <td>M</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.9485</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id Sex  Length  Diameter  Height  Whole_weight  Shucked_weight  \\\n",
       "0        0   M   0.455     0.365   0.095        0.5140          0.2245   \n",
       "1        1   M   0.350     0.265   0.090        0.2255          0.0995   \n",
       "2        2   F   0.530     0.420   0.135        0.6770          0.2565   \n",
       "3        3   M   0.440     0.365   0.125        0.5160          0.2155   \n",
       "4        4   I   0.330     0.255   0.080        0.2050          0.0895   \n",
       "...    ...  ..     ...       ...     ...           ...             ...   \n",
       "4172  4172   F   0.565     0.450   0.165        0.8870          0.3700   \n",
       "4173  4173   M   0.590     0.440   0.135        0.9660          0.4390   \n",
       "4174  4174   M   0.600     0.475   0.205        1.1760          0.5255   \n",
       "4175  4175   F   0.625     0.485   0.150        1.0945          0.5310   \n",
       "4176  4176   M   0.710     0.555   0.195        1.9485          0.9455   \n",
       "\n",
       "      Viscera_weight  Shell_weight  Rings  \n",
       "0             0.1010        0.1500     15  \n",
       "1             0.0485        0.0700      7  \n",
       "2             0.1415        0.2100      9  \n",
       "3             0.1140        0.1550     10  \n",
       "4             0.0395        0.0550      7  \n",
       "...              ...           ...    ...  \n",
       "4172          0.2390        0.2490     11  \n",
       "4173          0.2145        0.2605     10  \n",
       "4174          0.2875        0.3080      9  \n",
       "4175          0.2610        0.2960     10  \n",
       "4176          0.3765        0.4950     12  \n",
       "\n",
       "[4177 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('abalone.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hwgY2hbQRdG",
    "outputId": "105b7d68-c7ab-4247-cef9-f6e28cef643a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4177 entries, 0 to 4176\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              4177 non-null   int64  \n",
      " 1   Sex             4177 non-null   object \n",
      " 2   Length          4177 non-null   float64\n",
      " 3   Diameter        4177 non-null   float64\n",
      " 4   Height          4177 non-null   float64\n",
      " 5   Whole_weight    4177 non-null   float64\n",
      " 6   Shucked_weight  4177 non-null   float64\n",
      " 7   Viscera_weight  4177 non-null   float64\n",
      " 8   Shell_weight    4177 non-null   float64\n",
      " 9   Rings           4177 non-null   int64  \n",
      "dtypes: float64(7), int64(2), object(1)\n",
      "memory usage: 326.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# 데이터 파일 기본 정보 확인\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "pT6B4ZTzQY5D",
    "outputId": "47729d61-167e-4a33-dda6-3fb5e40e1abf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2088.000000</td>\n",
       "      <td>0.523992</td>\n",
       "      <td>0.407881</td>\n",
       "      <td>0.139516</td>\n",
       "      <td>0.828742</td>\n",
       "      <td>0.359367</td>\n",
       "      <td>0.180594</td>\n",
       "      <td>0.238831</td>\n",
       "      <td>9.933684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1205.940366</td>\n",
       "      <td>0.120093</td>\n",
       "      <td>0.099240</td>\n",
       "      <td>0.041827</td>\n",
       "      <td>0.490389</td>\n",
       "      <td>0.221963</td>\n",
       "      <td>0.109614</td>\n",
       "      <td>0.139203</td>\n",
       "      <td>3.224169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1044.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.441500</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2088.000000</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.799500</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3132.000000</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>1.153000</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4176.000000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>2.825500</td>\n",
       "      <td>1.488000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id       Length     Diameter       Height  Whole_weight  \\\n",
       "count  4177.000000  4177.000000  4177.000000  4177.000000   4177.000000   \n",
       "mean   2088.000000     0.523992     0.407881     0.139516      0.828742   \n",
       "std    1205.940366     0.120093     0.099240     0.041827      0.490389   \n",
       "min       0.000000     0.075000     0.055000     0.000000      0.002000   \n",
       "25%    1044.000000     0.450000     0.350000     0.115000      0.441500   \n",
       "50%    2088.000000     0.545000     0.425000     0.140000      0.799500   \n",
       "75%    3132.000000     0.615000     0.480000     0.165000      1.153000   \n",
       "max    4176.000000     0.815000     0.650000     1.130000      2.825500   \n",
       "\n",
       "       Shucked_weight  Viscera_weight  Shell_weight        Rings  \n",
       "count     4177.000000     4177.000000   4177.000000  4177.000000  \n",
       "mean         0.359367        0.180594      0.238831     9.933684  \n",
       "std          0.221963        0.109614      0.139203     3.224169  \n",
       "min          0.001000        0.000500      0.001500     1.000000  \n",
       "25%          0.186000        0.093500      0.130000     8.000000  \n",
       "50%          0.336000        0.171000      0.234000     9.000000  \n",
       "75%          0.502000        0.253000      0.329000    11.000000  \n",
       "max          1.488000        0.760000      1.005000    29.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpQMqKl2Qs3q",
    "outputId": "b2face7a-417f-4e85-cafa-14cbd813e9bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 결측치 확인 값이 False라면 Null 값이 없는거고 True면 있는거니깐\n",
    "# 해당 행을 제거해주거나 그 열에 해당하는 평균 값을 넣어주기\n",
    "\n",
    "df.isnull().values.any()\n",
    "\n",
    "# # 결측치가 있는 행을 제거하려면\n",
    "# df = df.dropna()\n",
    "\n",
    "# # 결측치를 열의 평균값으로 채우려면\n",
    "# df = df.fillna(df.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1HPTaQ1StkS"
   },
   "source": [
    "2-1. 문제에 맞는(지도 학습 -분류/회귀, 비지도 학습 - PCA/AutoEncoder) input, target으로 데이터 전처리 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hAtxXsnmTZe3"
   },
   "outputs": [],
   "source": [
    "# 전복 csv 데이터 기준으로, 분류 문제는 성별을 맞추는 것, 회귀 문제는 Rings 값을 예측하는 것으로 함\n",
    "\n",
    "# .values를 넣어 csv 데이터를 넘파이 배열로 바꿔줘야함\n",
    "\n",
    "# 분류인 경우 target 값은 성별이 됨.\n",
    "input = df.drop(['id','Sex'],axis=1).values\n",
    "target = df['Sex']\n",
    "\n",
    "# 회귀인 경우 target 값은 Rings가 됨.\n",
    "# input = df.drop(['id','Rings'],axis=1).values\n",
    "# target = df['Rings']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "B3HpaZPXU8c2"
   },
   "outputs": [],
   "source": [
    "# 단, 조건에 맞는 열만 가지고 문제를 수행하라 할 수 도 있음\n",
    "# 전복 데이터 중 Length, Height, Diameter 만 가지고 수행하라\n",
    "\n",
    "df = df[['Length','Height','Diameter']]\n",
    "\n",
    "# 분류 문제에서 target 값이 여러 개가 있을 수 있는데, 그 때 필요한 target 만 쓰라하면\n",
    "# isin 이라는 함수를 통해 원하는 값의 행 만 추출할 수 있음(전복의 성은 M,F,I 3개 가 있음)\n",
    "#df = df[df['Sex'].isin(['M', 'F'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eujIjsfkXl7s"
   },
   "source": [
    "2-2. 입력 데이터 또는 정답 데이터에 수치형 데이터가 아닌 범주형 데이터가 있다면 라벨 인코딩 또는 원-핫 인코딩을 수행해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "t0iwsuBwXwrN"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# target 을 수치형 데이터로 바꿔주기\n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(target)\n",
    "\n",
    "# 만약 input에 범주형 데이터가 있다면 필요한 열에 대해서 원-핫 인코딩을 수행해준다.\n",
    "# df = pd.get_dummies(df,columns=['Sex']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uHXZEy2feJ10"
   },
   "outputs": [],
   "source": [
    "# 만약 오토인코더 문제에서 'activity' 열 전처리: lyingBack, lyingRigh -> 정상 (0), sitting -> 비정상 (1)\n",
    "# 이러한 유형의 경우는 이렇게 replace를 쓰면 된다,\n",
    "\n",
    "# df['activity'] = df['activity'].replace({'lyingBack': 0, 'lyingRigh': 0, 'sitting': 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snRFkxvbfQK-"
   },
   "source": [
    "2-3. 내가 원하는 데이터를이 최종적으로 잘 추출되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sr5hvyc7fTvh"
   },
   "outputs": [],
   "source": [
    "# unique,shape,value_counts 로 확인 해보기\n",
    "\n",
    "# df['Sex'].unique(), df.shape, df['Sex'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfrAy8j-fpot"
   },
   "source": [
    "3-1. 데이터를 train,valid,test 로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KVNdh8zsfuF5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train/test 분리\n",
    "x_train ,x_test, y_train, y_test = train_test_split (input,target, test_size=0.2,stratify=target)\n",
    "# train/valid 분리\n",
    "x_train ,x_valid, y_train, y_valid = train_test_split (x_train,y_train, test_size=0.2,stratify=y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skck26l5grtb"
   },
   "source": [
    "3-2. 데이터의 정규화를 위해 StandardScaler 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "v2M9AiOdgw2F"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# train 에 대해서한 fit_transform을 적용하며, valid,test는 train에 맞게 transform만 하면 된다.\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_valid = ss.transform(x_valid)\n",
    "x_test = ss.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1YkP2XmhFiG"
   },
   "source": [
    "4-1. 모델 만들기 - 머신러닝 분류\n",
    "\n",
    "Logistic Regression, Decision Tree, Support Vector Classifier, KNN , Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "blmJhbk3j-aO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5526315789473685\n",
      "0.48564593301435405\n",
      "0.5394736842105263\n",
      "0.5370813397129187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jongmin\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5478468899521531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 로지스틱 리그레션\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "lr_pred = lr.predict(x_test)\n",
    "lr_acc = accuracy_score(y_test,lr_pred)\n",
    "print(lr_acc)\n",
    "\n",
    "# 의사결정트리\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train,y_train)\n",
    "dt_pred = dt.predict(x_test)\n",
    "dt_acc = accuracy_score(y_test,dt_pred)\n",
    "print(dt_acc)\n",
    "\n",
    "# 서포트 벡터\n",
    "svc = SVC()\n",
    "svc.fit(x_train,y_train)\n",
    "svc_pred = svc.predict(x_test)\n",
    "svc_acc = accuracy_score(y_test,svc_pred)\n",
    "print(svc_acc)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(x_train,y_train)\n",
    "knn_pred = knn.predict(x_test)\n",
    "knn_acc = accuracy_score(y_test,knn_pred)\n",
    "print(knn_acc)\n",
    "\n",
    "# 랜덤 포레스트\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train,y_train)\n",
    "rf_pred = rf.predict(x_test)\n",
    "rf_acc = accuracy_score(y_test,rf_pred)\n",
    "print(rf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM70DadCiulE"
   },
   "source": [
    "4-1. 모델 만들기- 머신러닝 회귀\n",
    "\n",
    "Linear Regression, Random Forest, Decision Tree,Support Vector Regression, KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WpjMW1cAkAJY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6732957479120851\n",
      "1.3648325358851674\n",
      "0.7748942473745098\n",
      "0.787177033492823\n",
      "0.7208627990430622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 선형 회귀\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "lr_pred = lr.predict(x_test)\n",
    "lr_mse = mean_squared_error(y_test,lr_pred)\n",
    "print(lr_mse)\n",
    "\n",
    "# 의사결정트리\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(x_train,y_train)\n",
    "dt_pred = dt.predict(x_test)\n",
    "dt_mse = mean_squared_error(y_test,dt_pred)\n",
    "print(dt_mse)\n",
    "\n",
    "# 서포트 벡터\n",
    "svr = SVR()\n",
    "svr.fit(x_train,y_train)\n",
    "svr_pred = svr.predict(x_test)\n",
    "svr_mse = mean_squared_error(y_test,svr_pred)\n",
    "print(svr_mse)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(x_train,y_train)\n",
    "knn_pred = knn.predict(x_test)\n",
    "knn_mse = mean_squared_error(y_test,knn_pred)\n",
    "print(knn_mse)\n",
    "\n",
    "# 랜덤 트리\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(x_train,y_train)\n",
    "rf_pred = rf.predict(x_test)\n",
    "rf_mse = mean_squared_error(y_test,rf_pred)\n",
    "print(rf_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6T3JD1dkEaL"
   },
   "source": [
    "4-1. 딥러닝 모델 만들기(Dense,CNN,LSTM,Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fvCruXeikOcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                576       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,689\n",
      "Trainable params: 2,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dense 모델\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "input_dim=x_train.shape[1]\n",
    "\n",
    "def create_dense_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # 이진 분류일 경우\n",
    "\n",
    "    # model.add(Dense(10, activation='softmax'))  # 다중 분류일 경우\n",
    "\n",
    "    # model.add(Dense(1))  # 회귀일 경우\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # 다중 분류일 경우(target을 원 핫 인코딩 한 경우)\n",
    "    # model.compile(optimizer='adam', loss='spares_categorical_crossentropy', metrics=['accuracy'])  # 다중 분류일 경우(target을 라벨 인코딩 한 경우)\n",
    "\n",
    "    # model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])  # 회귀일 경우\n",
    "    return model\n",
    "\n",
    "model = create_dense_model((input_dim,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PctVwrzymJuT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 6, 32)             128       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 3, 32)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 96)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                6208      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,401\n",
      "Trainable params: 6,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN 모델\n",
    "\n",
    "# CNN 모델은 기본적으로 3차원 이상의 데이터를 입력으로 받기 때문에\n",
    "# 필요에 맞게 입력 데이터의 차원을 늘려주어야 한다. 4차원Conv2D를 쓰려면 더 추가!\n",
    "\n",
    "# train,valid,test에 대해 동일하게 수행\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))  # 1D 합성곱\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # 이진 분류일 경우\n",
    "\n",
    "    # model.add(Dense(10, activation='softmax'))  # 다중 분류일 경우\n",
    "\n",
    "    # model.add(Dense(1))  # 회귀일 경우\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # 다중 분류일 경우(target을 원 핫 인코딩 한 경우)\n",
    "    # model.compile(optimizer='adam', loss='spares_categorical_crossentropy', metrics=['accuracy'])  # 다중 분류일 경우(target을 라벨 인코딩 한 경우)\n",
    "\n",
    "    # model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])  # 회귀일 경우\n",
    "    return model\n",
    "\n",
    "model = create_cnn_model((x_train.shape[1], 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NQSKd4jxqU2g"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_22544\\2836998896.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# 타깃값을 맨 뒤에 붙이기 그래야 함수안에 들어가서 x y 로 나눠줄수가 있음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "# LSTM 모델\n",
    "# 시계열 데이터가 필요하므로 데이터를 전처리 해줘야한다.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def split_sequences(sequences, n_steps):\n",
    "    x, y = list(), list()  # 빈 리스트를 생성하여 시퀀스 데이터와 레이블을 담을 공간을 만듦\n",
    "    for i in range(len(sequences)):  # 전체 시퀀스 데이터를 순회\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps  # 현재 인덱스(i)에서 n_steps만큼 떨어진 시퀀스의 끝을 계산\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):  # 시퀀스 끝이 데이터의 범위를 넘어서는지 확인\n",
    "            break  # 범위를 넘으면 루프 종료\n",
    "        # gather input (X) and output parts (y)\n",
    "        seq_x = sequences[i:end_ix, :-1]  # 입력 데이터 (특징 데이터)\n",
    "        seq_y_values = sequences[i:end_ix, -1]  # 시퀀스 동안의 출력 데이터 (레이블들)\n",
    "\n",
    "        # 가장 빈번하게 나온 레이블 찾기\n",
    "        most_common_label = Counter(seq_y_values).most_common(1)[0][0]\n",
    "\n",
    "        x.append(seq_x)  # 입력 데이터 추가\n",
    "        y.append(most_common_label)  # 가장 많이 나온 레이블 추가\n",
    "\n",
    "    return np.array(x), np.array(y)  # 리스트를 numpy 배열로 변환하여 반환.\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# 타깃값을 맨 뒤에 붙이기 그래야 함수안에 들어가서 x y 로 나눠줄수가 있음\n",
    "train_set = np.c_[x_train, y_train]\n",
    "\n",
    "n_steps = 3\n",
    "x_train, y_train = split_sequences(train_set, n_steps)\n",
    "\n",
    "# 이후 분류 문제에 따라 y_train을 인코딩해주기!\n",
    "\n",
    "# Define the input shape\n",
    "n_timesteps, n_features, n_outputs = x_train.shape[1], x_train.shape[2], y_train.shape[1]\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # 이진 분류일 경우\n",
    "\n",
    "    # model.add(Dense(10, activation='softmax'))  # 다중 분류일 경우\n",
    "\n",
    "    # model.add(Dense(1))  # 회귀일 경우\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # 다중 분류일 경우(target을 원 핫 인코딩 한 경우)\n",
    "    # model.compile(optimizer='adam', loss='spares_categorical_crossentropy', metrics=['accuracy'])  # 다중 분류일 경우(target을 라벨 인코딩 한 경우)\n",
    "\n",
    "    # model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])  # 회귀일 경우\n",
    "    return model\n",
    "\n",
    "model = create_lstm_model((n_timesteps, n_features))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xz3l_8O4wsis"
   },
   "outputs": [],
   "source": [
    "# 트랜스포터 모델, LSTM 모델과 같이 입력 3차원으로 넣어주기\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "# 문제에 맞는 손실함수 고를것\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4pg0KcjxvKn"
   },
   "source": [
    "4-2. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUj28OR4xxL_"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN_gi_6MyHGo"
   },
   "source": [
    "5-1. 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRWoSxwxyJo_"
   },
   "outputs": [],
   "source": [
    "# 손실 시각화\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, loss, 'b-', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r-', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIzXCSWGyPRw"
   },
   "outputs": [],
   "source": [
    "# 정확도 시각화\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, acc, 'b-', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r-', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oL_sPJiGyqaZ"
   },
   "outputs": [],
   "source": [
    "# 테스트 셋 평가\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1HC4MjdyugD"
   },
   "outputs": [],
   "source": [
    "# confusion matrix,# classification report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 모델 예측\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# 혼동 행렬 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 분류 리포트 출력\n",
    "cr = classification_report(y_test, y_pred_labels)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pip install --upgrade pip\\npip uninstall protobuf\\npip install protobuf==3.19.0\\n\\ndef cnn_lstm_model(input_shape):\\n    \\n    model = Sequential()\\n    model.add(Conv1D(32,kernel_size=1,activation='relu',input_shape=input_shape))\\n    model.add(MaxPooling1D(pool_size=1))\\n    model.add(LSTM(units=32))\\n    model.add(Dense(16, activation='relu'))\\n    model.add(Dense(n_outputs, activation='softmax'))\\n    \\n    model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\\n\\n    return model\\n\\ninput_shape = (n_timesteps,n_features)\\nmodel_2 = cnn_lstm_model(input_shape)\\n\\nmodel_2.summary()\\n\\nConv1D(32, kernel_size=1, activation='relu'): 1차원 합성곱 레이어로, 32개의 필터를 사용하고 kernel_size=1로 설정하였습니다. 활성화 함수로 relu를 사용합니다.\\nMaxPooling1D(pool_size=1): 1차원 최대 풀링 레이어로, 풀링 크기(pool_size)는 1로 설정되었습니다.\\nLSTM(units=32): 32개의 유닛을 가진 LSTM 레이어가 포함되어 있습니다.\\nDense(16, activation='relu'): 은닉층으로 16개의 뉴런을 사용하고, relu 활성화 함수를 사용합니다.\\nDense(n_outputs, activation='softmax'): 출력층으로, n_outputs는 분류할 클래스 수에 따라 설정됩니다. 출력층에서 softmax 활성화 함수를 사용하여 다중 클래스 분류를 수행합니다.\\nmodel.compile(): 옵티마이저는 Adam을 사용하며, 손실 함수로 categorical_crossentropy를 사용합니다. 모델은 다중 클래스 분류 문제를 해결하는 것으로 가정하고 있습니다.\\n실행 시 주의사항:\\n**n_timesteps**와 **n_features**는 입력 데이터에 따라 설정해야 합니다. 시계열 데이터의 경우 각각의 샘플 길이(시간 단계 수)와 각 샘플에 포함된 특성(피처)의 수를 의미합니다.\\n**n_outputs**는 분류해야 할 클래스의 개수를 나타내므로, 사용할 데이터셋에 따라 적절히 설정해야 합니다.\\n이 코드를 실행하면 CNN과 LSTM을 결합한 모델의 구조를 볼 수 있습니다.\\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n이 코드는 **CNN(1D 합성곱 신경망)**과 **LSTM(장기 단기 기억 신경망)**을 결합하여 시계열 데이터 또는 연속된 특성 데이터를 처리하기 위한 신경망 모델을 구성하는 예시입니다. 각 레이어가 어떤 역할을 하는지 단계별로 자세히 설명드리겠습니다.\\n\\n1. Sequential 모델 생성\\npython\\n코드 복사\\nmodel = Sequential()\\nSequential: 레이어를 순차적으로 쌓아 올릴 수 있는 모델입니다. 즉, 각 레이어는 순차적으로 앞의 레이어의 출력을 받아서 계산을 진행합니다. 여기서는 CNN과 LSTM을 차례대로 쌓아 올리는 구조를 만듭니다.\\n2. 1D 합성곱 레이어 (Conv1D)\\npython\\n코드 복사\\nmodel.add(Conv1D(32, kernel_size=1, activation='relu', input_shape=input_shape))\\nConv1D: 1차원 합성곱 레이어로, 주로 시계열 데이터나 연속된 특성 데이터(예: 신호 처리, 텍스트 데이터)에서 사용됩니다.\\n32: 32개의 필터를 사용합니다. 각 필터는 데이터를 처리하면서 특정 특징을 추출합니다.\\nkernel_size=1: 필터의 크기를 1로 설정합니다. 이는 각 데이터 포인트에서 독립적으로 특징을 추출하는 것을 의미합니다.\\nactivation='relu': 활성화 함수로 ReLU(Rectified Linear Unit)를 사용합니다. 이는 비선형성을 추가하여 신경망이 더 복잡한 관계를 학습할 수 있도록 합니다.\\ninput_shape=input_shape: 모델의 첫 번째 레이어이므로 입력 데이터의 형태를 지정해야 합니다. 여기서 input_shape는 (n_timesteps, n_features)로, **시계열 데이터의 길이(n_timesteps)**와 **각 타임스텝에서의 특성 수(n_features)**를 의미합니다.\\n3. 최대 풀링 레이어 (MaxPooling1D)\\npython\\n코드 복사\\nmodel.add(MaxPooling1D(pool_size=1))\\nMaxPooling1D: 최대 풀링 레이어는 합성곱 레이어의 출력에서 최대 값을 뽑아내는 역할을 합니다. 이는 데이터의 크기를 줄이면서 중요한 특징만 추출하여 계산량을 줄이고 과적합을 방지하는 데 도움이 됩니다.\\npool_size=1: 풀링의 크기를 1로 설정했습니다. 이 설정은 이 레이어가 데이터의 크기를 줄이지 않고, 데이터를 그대로 전달할 가능성이 큽니다. 실제로 더 큰 값으로 설정할 수 있지만, 여기서는 필터로만 특징을 추출하고 풀링은 하지 않는 셈입니다.\\n4. LSTM 레이어\\npython\\n코드 복사\\nmodel.add(LSTM(units=32))\\nLSTM(32): LSTM 레이어는 순환 신경망(RNN)의 일종으로, 시계열 데이터나 연속 데이터에서 중요한 과거 정보(장기 의존성)를 기억하는 능력이 있습니다. 특히, LSTM은 시퀀스 데이터에서 시간 의존적인 패턴을 학습하는 데 적합합니다.\\nunits=32: LSTM의 유닛(노드) 수를 32로 설정했습니다. LSTM의 유닛은 메모리 셀의 역할을 하며, 과거의 중요한 정보를 기억하는 역할을 합니다.\\n5. Dense (완전 연결층) 레이어\\npython\\n코드 복사\\nmodel.add(Dense(16, activation='relu'))\\nDense(16): **완전 연결층(FC, Fully Connected)**으로, 앞선 LSTM 레이어에서 나온 출력을 16개의 뉴런에 연결합니다.\\n16: 16개의 뉴런을 사용하여 데이터를 처리합니다.\\nactivation='relu': 활성화 함수로 ReLU를 사용하여 비선형성을 추가합니다.\\n6. 출력층 (Dense)\\npython\\n코드 복사\\nmodel.add(Dense(n_outputs, activation='softmax'))\\nDense(n_outputs): 최종 출력층으로, 예측할 클래스의 수만큼 뉴런을 만듭니다.\\nn_outputs: 출력 클래스의 수를 지정합니다. 예를 들어, 3개의 클래스가 있다면 n_outputs=3으로 설정해야 합니다.\\nactivation='softmax': 출력층의 활성화 함수로 소프트맥스 함수를 사용합니다. 이는 다중 클래스 분류 문제에서 각 클래스에 속할 확률을 계산해주는 함수입니다.\\n7. 모델 컴파일\\npython\\n코드 복사\\nmodel.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\\noptimizer='adam': 옵티마이저로 Adam을 사용합니다. Adam은 경사 하강법을 변형한 알고리즘으로, 빠르고 효과적인 학습을 제공합니다.\\nmetrics=['accuracy']: 모델의 성능을 평가할 때 사용할 지표로 **정확도(accuracy)**를 사용합니다.\\nloss='categorical_crossentropy': 손실 함수로 categorical crossentropy를 사용합니다. 이는 다중 클래스 분류 문제에서 주로 사용되는 손실 함수입니다.\\n8. 모델 요약 (summary)\\npython\\n코드 복사\\nmodel_2.summary()\\nsummary() 함수는 모델의 구조를 요약하여 보여줍니다. 각 레이어의 이름, 출력 형태, 그리고 학습 가능한 매개변수(파라미터)의 개수를 출력합니다.\\n모델 요약 내용 분석\\nConv1D (32 필터):\\n\\n출력 형태: (None, 4, 32) → 입력 데이터가 처리되어 32개의 특징 맵으로 변환되었음을 의미합니다.\\n파라미터 수: 288\\n계산: (입력 채널 수 x 커널 크기 x 필터 수) + 필터 수 = (1 x 1 x 32) + 32 = 288\\nMaxPooling1D:\\n\\n출력 형태: (None, 4, 32) → 풀링은 적용되지 않았고 데이터 크기는 변하지 않았습니다.\\n파라미터 수: 0 (풀링 레이어는 학습할 파라미터가 없습니다).\\nLSTM (32 유닛):\\n\\n출력 형태: (None, 32) → LSTM 레이어가 32개의 유닛을 출력합니다.\\n파라미터 수: 8,320\\n계산: (입력 수 + 상태 수 + 바이어스) * 4 * 유닛 수 = (32 + 32 + 1) * 4 * 32 = 8,320\\nDense (16 뉴런):\\n\\n출력 형태: (None, 16) → 16개의 뉴런으로 연결되어 출력됩니다.\\n파라미터 수: 528\\n계산: (입력 수 * 출력 수) + 바이어스 = (32 * 16) + 16 = 528\\nDense (출력층, n_outputs 뉴런):\\n\\n출력 형태: (None, n_outputs) → 분류할 클래스 수만큼 출력됩니다.\\n파라미터 수: 51\\n계산: (입력 수 * 출력 수) + 바이어스 = (16 * 3) + 3 = 51 (n_outputs = 3으로 가정)\\n결론\\n이 모델은 CNN과 LSTM을 결합하여 시계열 데이터나 연속된 특성을 처리하고 다중 클래스 분류를 수행합니다.\\n\\n**CNN(Conv1D)**은 특성 추출에 사용되며, 각 타임스텝에서 특징을 추출합니다.\\nLSTM은 시간에 따른 연속적인 정보를 학습하여 중요한 패턴을 기억합니다.\\n마지막 Dense 레이어를 통해 최종 분류 작업을 수행합니다.\\n이 모델은 시계열 분류, 텍스트 분류 또는 신호 처리 등의 분야에 적합합니다.\\n\\nMaxPooling1D(pool_size=1)**와 **MaxPooling1D(pool_size=2)**의 차이는 풀링 크기(pool size)에 있습니다. 풀링 크기는 입력 데이터에서 어느 정도의 영역을 하나의 값으로 줄일지를 결정하는 중요한 하이퍼파라미터입니다.\\n\\n1. MaxPooling1D(pool_size=1)\\n풀링 크기 1은 사실상 풀링을 하지 않는 것과 같은 의미입니다.\\n데이터의 크기를 줄이지 않고, 그대로 통과시키는 역할을 합니다.\\n각 입력 데이터 포인트는 변하지 않고 다음 레이어로 전달됩니다.\\n2. MaxPooling1D(pool_size=2)\\n풀링 크기 2는 인접한 2개의 데이터 포인트 중에서 최대값을 선택하여 출력을 줄입니다.\\n이로 인해 데이터의 크기가 절반으로 줄어듭니다. 예를 들어, 10개의 데이터 포인트가 있었다면, 풀링 후에는 5개의 데이터 포인트만 남습니다.\\n이 방식은 주로 중요한 특징을 추출하면서 데이터의 차원을 줄이고, 과적합을 방지하는 데 사용됩니다.\\n예시: 시계열 데이터 처리\\n입력 데이터: [3, 5, 7, 1, 9, 2] (총 6개의 값)\\n1. MaxPooling1D(pool_size=1)\\n출력: [3, 5, 7, 1, 9, 2] (원본 데이터가 그대로 유지됩니다)\\n데이터 크기는 변하지 않습니다.\\n2. MaxPooling1D(pool_size=2)\\n출력: [5, 7, 9] (2개의 값 중 최대값을 선택한 결과)\\n데이터 크기가 절반으로 줄어듭니다.\\n결론\\n**pool_size=1**은 데이터의 크기를 줄이지 않고 그대로 전달하는 반면,\\n**pool_size=2**는 입력 데이터의 인접한 2개의 값 중에서 최대값을 선택하여 데이터의 크기를 절반으로 줄입니다.\\n**pool_size=2**는 과적합을 방지하고, 계산 비용을 줄이며, 중요한 특징만 남기는 데 유용합니다.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pip install --upgrade pip\n",
    "pip uninstall protobuf\n",
    "pip install protobuf==3.19.0\n",
    "\n",
    "def cnn_lstm_model(input_shape):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32,kernel_size=1,activation='relu',input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(LSTM(units=32))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "input_shape = (n_timesteps,n_features)\n",
    "model_2 = cnn_lstm_model(input_shape)\n",
    "\n",
    "model_2.summary()\n",
    "\n",
    "Conv1D(32, kernel_size=1, activation='relu'): 1차원 합성곱 레이어로, 32개의 필터를 사용하고 kernel_size=1로 설정하였습니다. 활성화 함수로 relu를 사용합니다.\n",
    "MaxPooling1D(pool_size=1): 1차원 최대 풀링 레이어로, 풀링 크기(pool_size)는 1로 설정되었습니다.\n",
    "LSTM(units=32): 32개의 유닛을 가진 LSTM 레이어가 포함되어 있습니다.\n",
    "Dense(16, activation='relu'): 은닉층으로 16개의 뉴런을 사용하고, relu 활성화 함수를 사용합니다.\n",
    "Dense(n_outputs, activation='softmax'): 출력층으로, n_outputs는 분류할 클래스 수에 따라 설정됩니다. 출력층에서 softmax 활성화 함수를 사용하여 다중 클래스 분류를 수행합니다.\n",
    "model.compile(): 옵티마이저는 Adam을 사용하며, 손실 함수로 categorical_crossentropy를 사용합니다. 모델은 다중 클래스 분류 문제를 해결하는 것으로 가정하고 있습니다.\n",
    "실행 시 주의사항:\n",
    "**n_timesteps**와 **n_features**는 입력 데이터에 따라 설정해야 합니다. 시계열 데이터의 경우 각각의 샘플 길이(시간 단계 수)와 각 샘플에 포함된 특성(피처)의 수를 의미합니다.\n",
    "**n_outputs**는 분류해야 할 클래스의 개수를 나타내므로, 사용할 데이터셋에 따라 적절히 설정해야 합니다.\n",
    "이 코드를 실행하면 CNN과 LSTM을 결합한 모델의 구조를 볼 수 있습니다.\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "이 코드는 **CNN(1D 합성곱 신경망)**과 **LSTM(장기 단기 기억 신경망)**을 결합하여 시계열 데이터 또는 연속된 특성 데이터를 처리하기 위한 신경망 모델을 구성하는 예시입니다. 각 레이어가 어떤 역할을 하는지 단계별로 자세히 설명드리겠습니다.\n",
    "\n",
    "1. Sequential 모델 생성\n",
    "python\n",
    "코드 복사\n",
    "model = Sequential()\n",
    "Sequential: 레이어를 순차적으로 쌓아 올릴 수 있는 모델입니다. 즉, 각 레이어는 순차적으로 앞의 레이어의 출력을 받아서 계산을 진행합니다. 여기서는 CNN과 LSTM을 차례대로 쌓아 올리는 구조를 만듭니다.\n",
    "2. 1D 합성곱 레이어 (Conv1D)\n",
    "python\n",
    "코드 복사\n",
    "model.add(Conv1D(32, kernel_size=1, activation='relu', input_shape=input_shape))\n",
    "Conv1D: 1차원 합성곱 레이어로, 주로 시계열 데이터나 연속된 특성 데이터(예: 신호 처리, 텍스트 데이터)에서 사용됩니다.\n",
    "32: 32개의 필터를 사용합니다. 각 필터는 데이터를 처리하면서 특정 특징을 추출합니다.\n",
    "kernel_size=1: 필터의 크기를 1로 설정합니다. 이는 각 데이터 포인트에서 독립적으로 특징을 추출하는 것을 의미합니다.\n",
    "activation='relu': 활성화 함수로 ReLU(Rectified Linear Unit)를 사용합니다. 이는 비선형성을 추가하여 신경망이 더 복잡한 관계를 학습할 수 있도록 합니다.\n",
    "input_shape=input_shape: 모델의 첫 번째 레이어이므로 입력 데이터의 형태를 지정해야 합니다. 여기서 input_shape는 (n_timesteps, n_features)로, **시계열 데이터의 길이(n_timesteps)**와 **각 타임스텝에서의 특성 수(n_features)**를 의미합니다.\n",
    "3. 최대 풀링 레이어 (MaxPooling1D)\n",
    "python\n",
    "코드 복사\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "MaxPooling1D: 최대 풀링 레이어는 합성곱 레이어의 출력에서 최대 값을 뽑아내는 역할을 합니다. 이는 데이터의 크기를 줄이면서 중요한 특징만 추출하여 계산량을 줄이고 과적합을 방지하는 데 도움이 됩니다.\n",
    "pool_size=1: 풀링의 크기를 1로 설정했습니다. 이 설정은 이 레이어가 데이터의 크기를 줄이지 않고, 데이터를 그대로 전달할 가능성이 큽니다. 실제로 더 큰 값으로 설정할 수 있지만, 여기서는 필터로만 특징을 추출하고 풀링은 하지 않는 셈입니다.\n",
    "4. LSTM 레이어\n",
    "python\n",
    "코드 복사\n",
    "model.add(LSTM(units=32))\n",
    "LSTM(32): LSTM 레이어는 순환 신경망(RNN)의 일종으로, 시계열 데이터나 연속 데이터에서 중요한 과거 정보(장기 의존성)를 기억하는 능력이 있습니다. 특히, LSTM은 시퀀스 데이터에서 시간 의존적인 패턴을 학습하는 데 적합합니다.\n",
    "units=32: LSTM의 유닛(노드) 수를 32로 설정했습니다. LSTM의 유닛은 메모리 셀의 역할을 하며, 과거의 중요한 정보를 기억하는 역할을 합니다.\n",
    "5. Dense (완전 연결층) 레이어\n",
    "python\n",
    "코드 복사\n",
    "model.add(Dense(16, activation='relu'))\n",
    "Dense(16): **완전 연결층(FC, Fully Connected)**으로, 앞선 LSTM 레이어에서 나온 출력을 16개의 뉴런에 연결합니다.\n",
    "16: 16개의 뉴런을 사용하여 데이터를 처리합니다.\n",
    "activation='relu': 활성화 함수로 ReLU를 사용하여 비선형성을 추가합니다.\n",
    "6. 출력층 (Dense)\n",
    "python\n",
    "코드 복사\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "Dense(n_outputs): 최종 출력층으로, 예측할 클래스의 수만큼 뉴런을 만듭니다.\n",
    "n_outputs: 출력 클래스의 수를 지정합니다. 예를 들어, 3개의 클래스가 있다면 n_outputs=3으로 설정해야 합니다.\n",
    "activation='softmax': 출력층의 활성화 함수로 소프트맥스 함수를 사용합니다. 이는 다중 클래스 분류 문제에서 각 클래스에 속할 확률을 계산해주는 함수입니다.\n",
    "7. 모델 컴파일\n",
    "python\n",
    "코드 복사\n",
    "model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "optimizer='adam': 옵티마이저로 Adam을 사용합니다. Adam은 경사 하강법을 변형한 알고리즘으로, 빠르고 효과적인 학습을 제공합니다.\n",
    "metrics=['accuracy']: 모델의 성능을 평가할 때 사용할 지표로 **정확도(accuracy)**를 사용합니다.\n",
    "loss='categorical_crossentropy': 손실 함수로 categorical crossentropy를 사용합니다. 이는 다중 클래스 분류 문제에서 주로 사용되는 손실 함수입니다.\n",
    "8. 모델 요약 (summary)\n",
    "python\n",
    "코드 복사\n",
    "model_2.summary()\n",
    "summary() 함수는 모델의 구조를 요약하여 보여줍니다. 각 레이어의 이름, 출력 형태, 그리고 학습 가능한 매개변수(파라미터)의 개수를 출력합니다.\n",
    "모델 요약 내용 분석\n",
    "Conv1D (32 필터):\n",
    "\n",
    "출력 형태: (None, 4, 32) → 입력 데이터가 처리되어 32개의 특징 맵으로 변환되었음을 의미합니다.\n",
    "파라미터 수: 288\n",
    "계산: (입력 채널 수 x 커널 크기 x 필터 수) + 필터 수 = (1 x 1 x 32) + 32 = 288\n",
    "MaxPooling1D:\n",
    "\n",
    "출력 형태: (None, 4, 32) → 풀링은 적용되지 않았고 데이터 크기는 변하지 않았습니다.\n",
    "파라미터 수: 0 (풀링 레이어는 학습할 파라미터가 없습니다).\n",
    "LSTM (32 유닛):\n",
    "\n",
    "출력 형태: (None, 32) → LSTM 레이어가 32개의 유닛을 출력합니다.\n",
    "파라미터 수: 8,320\n",
    "계산: (입력 수 + 상태 수 + 바이어스) * 4 * 유닛 수 = (32 + 32 + 1) * 4 * 32 = 8,320\n",
    "Dense (16 뉴런):\n",
    "\n",
    "출력 형태: (None, 16) → 16개의 뉴런으로 연결되어 출력됩니다.\n",
    "파라미터 수: 528\n",
    "계산: (입력 수 * 출력 수) + 바이어스 = (32 * 16) + 16 = 528\n",
    "Dense (출력층, n_outputs 뉴런):\n",
    "\n",
    "출력 형태: (None, n_outputs) → 분류할 클래스 수만큼 출력됩니다.\n",
    "파라미터 수: 51\n",
    "계산: (입력 수 * 출력 수) + 바이어스 = (16 * 3) + 3 = 51 (n_outputs = 3으로 가정)\n",
    "결론\n",
    "이 모델은 CNN과 LSTM을 결합하여 시계열 데이터나 연속된 특성을 처리하고 다중 클래스 분류를 수행합니다.\n",
    "\n",
    "**CNN(Conv1D)**은 특성 추출에 사용되며, 각 타임스텝에서 특징을 추출합니다.\n",
    "LSTM은 시간에 따른 연속적인 정보를 학습하여 중요한 패턴을 기억합니다.\n",
    "마지막 Dense 레이어를 통해 최종 분류 작업을 수행합니다.\n",
    "이 모델은 시계열 분류, 텍스트 분류 또는 신호 처리 등의 분야에 적합합니다.\n",
    "\n",
    "MaxPooling1D(pool_size=1)**와 **MaxPooling1D(pool_size=2)**의 차이는 풀링 크기(pool size)에 있습니다. 풀링 크기는 입력 데이터에서 어느 정도의 영역을 하나의 값으로 줄일지를 결정하는 중요한 하이퍼파라미터입니다.\n",
    "\n",
    "1. MaxPooling1D(pool_size=1)\n",
    "풀링 크기 1은 사실상 풀링을 하지 않는 것과 같은 의미입니다.\n",
    "데이터의 크기를 줄이지 않고, 그대로 통과시키는 역할을 합니다.\n",
    "각 입력 데이터 포인트는 변하지 않고 다음 레이어로 전달됩니다.\n",
    "2. MaxPooling1D(pool_size=2)\n",
    "풀링 크기 2는 인접한 2개의 데이터 포인트 중에서 최대값을 선택하여 출력을 줄입니다.\n",
    "이로 인해 데이터의 크기가 절반으로 줄어듭니다. 예를 들어, 10개의 데이터 포인트가 있었다면, 풀링 후에는 5개의 데이터 포인트만 남습니다.\n",
    "이 방식은 주로 중요한 특징을 추출하면서 데이터의 차원을 줄이고, 과적합을 방지하는 데 사용됩니다.\n",
    "예시: 시계열 데이터 처리\n",
    "입력 데이터: [3, 5, 7, 1, 9, 2] (총 6개의 값)\n",
    "1. MaxPooling1D(pool_size=1)\n",
    "출력: [3, 5, 7, 1, 9, 2] (원본 데이터가 그대로 유지됩니다)\n",
    "데이터 크기는 변하지 않습니다.\n",
    "2. MaxPooling1D(pool_size=2)\n",
    "출력: [5, 7, 9] (2개의 값 중 최대값을 선택한 결과)\n",
    "데이터 크기가 절반으로 줄어듭니다.\n",
    "결론\n",
    "**pool_size=1**은 데이터의 크기를 줄이지 않고 그대로 전달하는 반면,\n",
    "**pool_size=2**는 입력 데이터의 인접한 2개의 값 중에서 최대값을 선택하여 데이터의 크기를 절반으로 줄입니다.\n",
    "**pool_size=2**는 과적합을 방지하고, 계산 비용을 줄이며, 중요한 특징만 남기는 데 유용합니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qNNN5ajzXVS"
   },
   "source": [
    "6. 성능 개량"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJXJVVYfzY9g"
   },
   "source": [
    "추가적인 성능을 높이는데는 드롭아웃, 배치 정규화, 밀집 연결층 추가 또는 삭제, 유닛 수 늘리기, 사전 학습 모델로 만들고 층 동결등 미세 조정 등이 있음."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
